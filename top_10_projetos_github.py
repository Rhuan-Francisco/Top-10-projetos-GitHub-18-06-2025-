# -*- coding: utf-8 -*-
"""Top 10 projetos GitHub (18/06/2025).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QfIslqKYoQ_c2R1sIrLGHlkygsJhdF7X

* Faz as importações
"""

import pandas as pd
from bs4 import BeautifulSoup
import requests
from requests.exceptions import HTTPError
import re

"""* Função para fazer Crawling"""

def crawl_website(url: str) -> str:

  """
    Função que realiza o crawling (rastreamento) de uma página web e retorna seu conteúdo HTML.

    Args:
      url (str): Endereço da página web a ser acessada.

    Returns:
    str: Conteúdo HTML da página se a requisição for bem-sucedida.
      None se ocorrer um erro.
  """

  # Cabeçalhos HTTP para simular um navegador e definir preferências de idioma
  HEADERS = {
    'User-Agent':'Mozila/5.0',
    'Accept-Language':'en-US,en;q=0.5'
}

  try:

    # Faz a requisição GET para a URL com os cabeçalhos definidos
    resposta = requests.get(url=URL, headers=HEADERS)
    # Levanta uma exceção se a resposta tiver status code de erro (4xx ou 5xx)
    resposta.raise_for_status()

  except HTTPError as exc:
    # Trata erros de HTTP (como 404, 500, etc.)
    print(exc)
    return None

  else:
    # Retorna o conteúdo HTML/texto da página se a requisição for bem-sucedida
    return resposta.text

"""* Faz o WebScraping"""

URL = 'https://github.com/trending'

conteudo = crawl_website(url  = URL)

# Salva o conteudo em um arquivo local
with open('github.html',mode ='w', encoding = 'utf8') as arquivo:
  arquivo.write(conteudo)

pagina_github = BeautifulSoup(open( file = 'github.html', mode = 'r'), 'html.parser')

"""* Separação de conteudo"""

conteudo_extraido = []

# lista de palavras para excluir
excluir = ['Sponsor', 'Star','']

projetos = pagina_github.find_all('article', class_ = 'Box-row')

for projeto in projetos[:10]:

  # Extrai todo o texto
  textos_coluna = projeto.getText(';')

  texto_processado = []

  for frase in textos_coluna.split(';'):
    frase = frase.strip()

    # pula frases vazias
    if not frase:
      continue

    # divide em palavras, remove as indesejadas e junta novamente
    palavras = frase.split()
    palavras_filtradas = [p for p in palavras if p not in excluir]
    frase_filtrada = ' '.join(palavras_filtradas)

    # se tiver algum conteudo na frase, adiciona na lista
    if frase_filtrada != '':
      texto_processado.append(frase_filtrada)

    # remove os espaços em branco indesejados
    texto_processado = [frase for frase in texto_processado if frase.strip()  != '']

  # Adiciona a lista apenas se não estiver vazio
  if texto_processado:
    conteudo_extraido.append(texto_processado)

for linha in conteudo_extraido:
  print(linha)

"""* Agora com os dados separados, construimos um dicionario somente com os dados que queremos."""

projetos_github = []

for conteudo in conteudo_extraido:
  github = {
    'Projeto': conteudo[1],
    'Linguagem': conteudo[3],
    'Estrelas': conteudo[4],
    'Forks': conteudo[5],
  }

  projetos_github.append(github)

for projeto in projetos_github:
  print(projeto)

"""* Aqui usamos o dicionario para criarmos um DataFrame, separando as informações que queremos."""

projetos_github_df = pd.DataFrame(projetos_github)
projetos_github_df.head(10)